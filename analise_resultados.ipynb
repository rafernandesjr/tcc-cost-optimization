{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71780355",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from pathlib import Path\n",
    "from typing import Tuple, Dict, Any\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "CSV_PATH = Path(\"runs.csv\")\n",
    "OUT_DIR  = Path(\"metricas\")\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "pd.set_option(\"display.max_columns\", 200)\n",
    "pd.set_option(\"display.width\", 140)\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "df.head(1)\n",
    "print(\"Linhas x colunas:\", df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d02d1069",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_state(s):\n",
    "    if pd.isna(s): return np.nan\n",
    "    su = str(s).upper()\n",
    "    if \"DONE\" in su: return \"DONE\"\n",
    "    if \"CANCEL\" in su: return \"CANCELLED\"\n",
    "    if \"FAIL\" in su or \"ERROR\" in su: return \"FAILED\"\n",
    "    if \"RUNNING\" in su: return \"RUNNING\"\n",
    "    return str(s)\n",
    "\n",
    "df[\"state_norm\"] = df.get(\"state\", np.nan).apply(normalize_state)\n",
    "df[\"success\"]    = df[\"state_norm\"].eq(\"DONE\")\n",
    "\n",
    "for col in [\"timestamp_utc\",\"start_ts\",\"end_ts\"]:\n",
    "    if col in df.columns:\n",
    "        df[col] = pd.to_datetime(df[col], errors=\"coerce\")\n",
    "\n",
    "num_cols = [\n",
    "    \"duration_s\",\"throughput_rps\",\"records_processed\",\n",
    "    \"vcpu_hours\",\"memory_gb_hours\",\"pd_gb_hours\",\"ssd_gb_hours\",\n",
    "    \"shuffle_gb\",\"total_shuffle_gb\",\"current_vcpus\",\"current_memory_gb\",\n",
    "    \"cost_vcpu_usd\",\"cost_memory_usd\",\"cost_shuffle_usd\",\n",
    "    \"total_cost_usd\",\"unit_cost_usd_per_mm\",\n",
    "    \"num_workers\",\"max_num_workers\"\n",
    "]\n",
    "for c in num_cols:\n",
    "    if c in df.columns:\n",
    "        df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "\n",
    "def infer_test(row):\n",
    "    for k in [\"test\",\"experiment\"]:\n",
    "        if k in row and pd.notna(row[k]):\n",
    "            return str(row[k]).upper()\n",
    "    import re\n",
    "    m = re.search(r\"(t\\d+)\", str(row.get(\"job_name\",\"\")), re.IGNORECASE)\n",
    "    return m.group(1).upper() if m else np.nan\n",
    "\n",
    "if \"test\" not in df.columns:\n",
    "    df[\"test\"] = df.apply(infer_test, axis=1)\n",
    "else:\n",
    "    df[\"test\"] = df[\"test\"].astype(str).str.upper()\n",
    "\n",
    "if \"variant\" in df.columns:\n",
    "    df[\"variant\"] = df[\"variant\"].astype(str).str.lower()\n",
    "\n",
    "expected_min = [\"test\",\"variant\",\"duration_s\",\"throughput_rps\",\"records_processed\",\n",
    "                \"total_cost_usd\",\"unit_cost_usd_per_mm\",\"state_norm\",\"success\"]\n",
    "audit = pd.DataFrame([(c, c in df.columns, (str(df[c].dtype) if c in df.columns else \"-\"))\n",
    "                      for c in expected_min], columns=[\"column\",\"present\",\"dtype\"])\n",
    "audit_path = OUT_DIR / \"audit_core_columns.csv\"\n",
    "audit.to_csv(audit_path, index=False)\n",
    "audit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59743dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ci_mean_t(a: np.ndarray, alpha: float = 0.05) -> Tuple[float,float,float,int]:\n",
    "    \"\"\"Média e IC 95% via t-Student (robusto para n pequeno).\"\"\"\n",
    "    a = a[~np.isnan(a)]\n",
    "    n  = a.size\n",
    "    m  = float(np.mean(a)) if n > 0 else np.nan\n",
    "    sd = float(np.std(a, ddof=1)) if n > 1 else np.nan\n",
    "    if n <= 1 or not math.isfinite(sd):\n",
    "        return m, np.nan, np.nan, n\n",
    "    tcrit = stats.t.ppf(1 - alpha/2, df=n-1)\n",
    "    half  = tcrit * sd / math.sqrt(n)\n",
    "    return m, m - half, m + half, n\n",
    "\n",
    "def try_normality(a: np.ndarray) -> float:\n",
    "    \"\"\"Shapiro–Wilk, retorna p-valor (n>=5); NaN caso contrário.\"\"\"\n",
    "    a = a[~np.isnan(a)]\n",
    "    if a.size < 5:\n",
    "        return np.nan\n",
    "    try:\n",
    "        return stats.shapiro(a).pvalue\n",
    "    except Exception:\n",
    "        return np.nan\n",
    "\n",
    "def test_two_groups(x: np.ndarray, y: np.ndarray) -> Dict[str, Any]:\n",
    "    \"\"\"Welch t-test se normalidade plausível e n>=5; senão Mann–Whitney.\n",
    "       Retorna teste usado, estatística, p-valor e tamanho de efeito.\"\"\"\n",
    "    x = x[~np.isnan(x)]\n",
    "    y = y[~np.isnan(y)]\n",
    "    nx, ny = x.size, y.size\n",
    "    res = {\"nx\": int(nx), \"ny\": int(ny)}\n",
    "\n",
    "    if nx < 2 or ny < 2:\n",
    "        res.update({\"test\":\"insuficiente\", \"stat\":np.nan, \"pvalue\":np.nan,\n",
    "                    \"effect\":np.nan, \"effect_name\":\"-\"})\n",
    "        return res\n",
    "\n",
    "    px = try_normality(x)\n",
    "    py = try_normality(y)\n",
    "    normal_enough = (not np.isnan(px) and px > 0.05) and (not np.isnan(py) and py > 0.05)\n",
    "\n",
    "    if normal_enough and nx >= 5 and ny >= 5:\n",
    "        # Welch\n",
    "        t, p = stats.ttest_ind(x, y, equal_var=False)\n",
    "        # Cohen's d\n",
    "        sdx = np.var(x, ddof=1); sdy = np.var(y, ddof=1)\n",
    "        sp  = np.sqrt(((nx-1)*sdx + (ny-1)*sdy) / (nx+ny-2)) if (nx+ny-2)>0 else np.nan\n",
    "        d   = (np.mean(x) - np.mean(y)) / sp if (sp and sp>0) else np.nan\n",
    "        res.update({\"test\":\"Welch t\", \"stat\":float(t), \"pvalue\":float(p),\n",
    "                    \"effect\":float(d), \"effect_name\":\"Cohen d\"})\n",
    "    else:\n",
    "        # Mann–Whitney\n",
    "        u, p = stats.mannwhitneyu(x, y, alternative=\"two-sided\")\n",
    "        r_rb = 1 - 2*u/(nx*ny)  # rank-biserial\n",
    "        res.update({\"test\":\"Mann-Whitney\", \"stat\":float(u), \"pvalue\":float(p),\n",
    "                    \"effect\":float(r_rb), \"effect_name\":\"rank-biserial\"})\n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e3995d",
   "metadata": {},
   "outputs": [],
   "source": [
    "group_cols = [\"test\",\"variant\"]\n",
    "agg = {\n",
    "    \"success\": [\"count\",\"sum\"],\n",
    "    \"duration_s\": [\"mean\",\"std\",\"min\",\"max\",\"median\"],\n",
    "    \"throughput_rps\": [\"mean\",\"std\",\"min\",\"max\",\"median\"],\n",
    "    \"records_processed\": [\"mean\",\"sum\"],\n",
    "    \"total_cost_usd\": [\"mean\",\"std\",\"min\",\"max\",\"median\",\"sum\"],\n",
    "    \"unit_cost_usd_per_mm\": [\"mean\",\"std\",\"min\",\"max\",\"median\"],\n",
    "    \"vcpu_hours\": [\"mean\",\"sum\"],\n",
    "    \"memory_gb_hours\": [\"mean\",\"sum\"],\n",
    "    \"shuffle_gb\": [\"mean\",\"sum\"]\n",
    "}\n",
    "present_agg = {k:v for k,v in agg.items() if k in df.columns}\n",
    "\n",
    "summary = df.groupby(group_cols, dropna=False).agg(present_agg)\n",
    "summary.columns = [\"_\".join([c for c in col if c]).strip(\"_\") for col in summary.columns.values]\n",
    "summary = summary.reset_index().rename(columns={\n",
    "    \"success_count\":\"n_runs\", \"success_sum\":\"n_sucessos\",\n",
    "    \"duration_s_mean\":\"dur_mean_s\",\"duration_s_std\":\"dur_std_s\",\n",
    "    \"duration_s_min\":\"dur_min_s\",\"duration_s_max\":\"dur_max_s\",\"duration_s_median\":\"dur_mediana_s\",\n",
    "    \"throughput_rps_mean\":\"thr_mean_rps\",\"throughput_rps_std\":\"thr_std_rps\",\n",
    "    \"throughput_rps_min\":\"thr_min_rps\",\"throughput_rps_max\":\"thr_max_rps\",\"throughput_rps_median\":\"thr_mediana_rps\",\n",
    "    \"records_processed_mean\":\"reg_mean\",\"records_processed_sum\":\"reg_total\",\n",
    "    \"total_cost_usd_mean\":\"custo_medio_usd\",\"total_cost_usd_std\":\"custo_std_usd\",\n",
    "    \"total_cost_usd_min\":\"custo_min_usd\",\"total_cost_usd_max\":\"custo_max_usd\",\"total_cost_usd_median\":\"custo_mediana_usd\",\n",
    "    \"total_cost_usd_sum\":\"custo_total_usd\",\n",
    "    \"unit_cost_usd_per_mm_mean\":\"custo_unit_mean_usd_por_mm\",\"unit_cost_usd_per_mm_std\":\"custo_unit_std_usd_por_mm\",\n",
    "    \"unit_cost_usd_per_mm_min\":\"custo_unit_min_usd_por_mm\",\"unit_cost_usd_per_mm_max\":\"custo_unit_max_usd_por_mm\",\n",
    "    \"unit_cost_usd_per_mm_median\":\"custo_unit_mediana_usd_por_mm\",\n",
    "    \"vcpu_hours_mean\":\"vcpu_mean_h\",\"vcpu_hours_sum\":\"vcpu_total_h\",\n",
    "    \"memory_gb_hours_mean\":\"mem_mean_gb_h\",\"memory_gb_hours_sum\":\"mem_total_gb_h\",\n",
    "    \"shuffle_gb_mean\":\"shuffle_mean_gb\",\"shuffle_gb_sum\":\"shuffle_total_gb\"\n",
    "})\n",
    "if \"n_runs\" in summary.columns and \"n_sucessos\" in summary.columns:\n",
    "    summary[\"taxa_sucesso\"] = summary[\"n_sucessos\"] / summary[\"n_runs\"]\n",
    "\n",
    "summary_path = OUT_DIR / \"resumo_teste_variante.csv\"\n",
    "summary.to_csv(summary_path, index=False)\n",
    "summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e71b7af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nível Dataset (opcional): Teste–Variante–Dataset\n",
    "if \"dataset_tag\" in df.columns:\n",
    "    gds = [\"test\",\"variant\",\"dataset_tag\"]\n",
    "    summary_ds = df.groupby(gds, dropna=False).agg(present_agg)\n",
    "    summary_ds.columns = [\"_\".join([c for c in col if c]).strip(\"_\") for col in summary_ds.columns.values]\n",
    "    summary_ds = summary_ds.reset_index().rename(columns={\n",
    "        \"success_count\":\"n_runs\", \"success_sum\":\"n_sucessos\",\n",
    "        \"duration_s_mean\":\"dur_mean_s\",\"duration_s_std\":\"dur_std_s\",\n",
    "        \"duration_s_min\":\"dur_min_s\",\"duration_s_max\":\"dur_max_s\",\"duration_s_median\":\"dur_mediana_s\",\n",
    "        \"throughput_rps_mean\":\"thr_mean_rps\",\"throughput_rps_std\":\"thr_std_rps\",\n",
    "        \"throughput_rps_min\":\"thr_min_rps\",\"throughput_rps_max\":\"thr_max_rps\",\"throughput_rps_median\":\"thr_mediana_rps\",\n",
    "        \"records_processed_mean\":\"reg_mean\",\"records_processed_sum\":\"reg_total\",\n",
    "        \"total_cost_usd_mean\":\"custo_medio_usd\",\"total_cost_usd_std\":\"custo_std_usd\",\n",
    "        \"total_cost_usd_min\":\"custo_min_usd\",\"total_cost_usd_max\":\"custo_max_usd\",\"total_cost_usd_median\":\"custo_mediana_usd\",\n",
    "        \"total_cost_usd_sum\":\"custo_total_usd\",\n",
    "        \"unit_cost_usd_per_mm_mean\":\"custo_unit_mean_usd_por_mm\",\"unit_cost_usd_per_mm_std\":\"custo_unit_std_usd_por_mm\",\n",
    "        \"unit_cost_usd_per_mm_min\":\"custo_unit_min_usd_por_mm\",\"unit_cost_usd_per_mm_max\":\"custo_unit_max_usd_por_mm\",\n",
    "        \"unit_cost_usd_per_mm_median\":\"custo_unit_mediana_usd_por_mm\",\n",
    "        \"vcpu_hours_mean\":\"vcpu_mean_h\",\"vcpu_hours_sum\":\"vcpu_total_h\",\n",
    "        \"memory_gb_hours_mean\":\"mem_mean_gb_h\",\"memory_gb_hours_sum\":\"mem_total_gb_h\",\n",
    "        \"shuffle_gb_mean\":\"shuffle_mean_gb\",\"shuffle_gb_sum\":\"shuffle_total_gb\"\n",
    "    })\n",
    "    summary_ds_path = OUT_DIR / \"resumo_teste_variante_dataset.csv\"\n",
    "    summary_ds.to_csv(summary_ds_path, index=False)\n",
    "    summary_ds.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebaa7b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ci_table(df_in: pd.DataFrame, group_cols, metric: str) -> pd.DataFrame:\n",
    "    rows = []\n",
    "    for keys, part in df_in.groupby(group_cols, dropna=False):\n",
    "        a = part[metric].to_numpy(dtype=float)\n",
    "        mean, lo, hi, n = ci_mean_t(a, alpha=0.05)\n",
    "        row = dict(zip(group_cols, keys if isinstance(keys, tuple) else (keys,)))\n",
    "        row.update({\"metric\": metric, \"n\": n, \"mean\": mean, \"ci95_lo\": lo, \"ci95_hi\": hi})\n",
    "        rows.append(row)\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "ci_metrics = [\"duration_s\",\"throughput_rps\",\"total_cost_usd\",\"unit_cost_usd_per_mm\"]\n",
    "ci_frames = []\n",
    "for m in ci_metrics:\n",
    "    if m in df.columns:\n",
    "        ci_frames.append(ci_table(df, [\"test\",\"variant\"], m))\n",
    "ci_all = pd.concat(ci_frames, ignore_index=True) if ci_frames else pd.DataFrame()\n",
    "ci_path = OUT_DIR / \"ci95_por_teste_variante.csv\"\n",
    "ci_all.to_csv(ci_path, index=False)\n",
    "ci_all.head(12)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "425f7b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pairwise_tests_per_test(df_in: pd.DataFrame, metric: str) -> pd.DataFrame:\n",
    "    out = []\n",
    "    for test_name, g in df_in.groupby(\"test\"):\n",
    "        variants = list(g[\"variant\"].dropna().unique())\n",
    "        if len(variants) != 2:\n",
    "            continue  # esperado 2 variantes por teste\n",
    "        v1, v2 = variants[0], variants[1]\n",
    "        x = g.loc[g[\"variant\"]==v1, metric].to_numpy(dtype=float)\n",
    "        y = g.loc[g[\"variant\"]==v2, metric].to_numpy(dtype=float)\n",
    "        res = test_two_groups(x, y)\n",
    "        out.append({\n",
    "            \"test\": test_name, \"metric\": metric,\n",
    "            \"variant_a\": v1, \"variant_b\": v2,\n",
    "            \"nx\": res[\"nx\"], \"ny\": res[\"ny\"],\n",
    "            \"test_used\": res[\"test\"], \"stat\": res[\"stat\"],\n",
    "            \"pvalue\": res[\"pvalue\"], \"effect\": res[\"effect\"], \"effect_name\": res[\"effect_name\"],\n",
    "            \"mean_a\": float(np.nanmean(x)) if x.size>0 else np.nan,\n",
    "            \"mean_b\": float(np.nanmean(y)) if y.size>0 else np.nan,\n",
    "            \"delta_abs\": (float(np.nanmean(x)) - float(np.nanmean(y))) if x.size>0 and y.size>0 else np.nan,\n",
    "            \"delta_pct\": ((float(np.nanmean(x)) / float(np.nanmean(y)) - 1)*100.0)\n",
    "                         if x.size>0 and y.size>0 and np.nanmean(y)!=0 else np.nan\n",
    "        })\n",
    "    return pd.DataFrame(out)\n",
    "\n",
    "test_metrics = [\"duration_s\",\"throughput_rps\",\"total_cost_usd\",\"unit_cost_usd_per_mm\"]\n",
    "tests_all = []\n",
    "for m in test_metrics:\n",
    "    if m in df.columns:\n",
    "        tests_all.append(pairwise_tests_per_test(df, m))\n",
    "tests_all = pd.concat(tests_all, ignore_index=True) if tests_all else pd.DataFrame()\n",
    "tests_path = OUT_DIR / \"tests_pairwise_por_teste.csv\"\n",
    "tests_all.to_csv(tests_path, index=False)\n",
    "tests_all\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "834d7a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_pairwise_per_test(df_summary, test_name, metrics):\n",
    "    sub = df_summary[df_summary[\"test\"]==test_name]\n",
    "    for col, title, ylabel, fname in metrics:\n",
    "        if col not in sub.columns: continue\n",
    "        plt.figure(figsize=(6,4))\n",
    "        plt.bar(sub[\"variant\"], sub[col].values)\n",
    "        plt.ylabel(ylabel)\n",
    "        plt.title(f\"{test_name} — {title}\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(OUT_DIR / f\"{test_name}_{fname}.png\")\n",
    "        plt.close()\n",
    "\n",
    "metrics = [\n",
    "    (\"dur_mean_s\", \"Duração média (s)\", \"s\", \"duracao.png\"),\n",
    "    (\"thr_mean_rps\", \"Throughput médio (reg/s)\", \"reg/s\", \"throughput.png\"),\n",
    "    (\"custo_unit_mean_usd_por_mm\", \"Custo unitário médio (USD por 1M)\", \"USD/1M\", \"custo_unit.png\"),\n",
    "]\n",
    "\n",
    "for t in summary[\"test\"].unique():\n",
    "    plot_pairwise_per_test(summary, t, metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a65aa6da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def percent_diffs(df_in: pd.DataFrame, metric: str) -> pd.DataFrame:\n",
    "    res = []\n",
    "    for t, g in df_in.groupby(\"test\"):\n",
    "        part = g[[\"variant\", metric]].dropna()\n",
    "        if part.empty or part[\"variant\"].nunique()!=2:\n",
    "            continue\n",
    "        means = part.groupby(\"variant\")[metric].mean()\n",
    "        pairs = means.index.tolist()\n",
    "        if len(pairs)!=2:\n",
    "            continue\n",
    "        a, b = pairs[0], pairs[1]\n",
    "        ma, mb = means[a], means[b]\n",
    "        res.append({\n",
    "            \"test\": t, \"metric\": metric,\n",
    "            \"variant_a\": a, \"mean_a\": ma,\n",
    "            \"variant_b\": b, \"mean_b\": mb,\n",
    "            \"delta_abs\": ma - mb,\n",
    "            \"delta_pct\": ((ma/mb - 1)*100.0) if mb!=0 else np.nan\n",
    "        })\n",
    "    return pd.DataFrame(res)\n",
    "\n",
    "pct_frames = []\n",
    "for m in [\"duration_s\",\"throughput_rps\",\"total_cost_usd\",\"unit_cost_usd_per_mm\"]:\n",
    "    if m in df.columns:\n",
    "        pct_frames.append(percent_diffs(df, m))\n",
    "pct_all = pd.concat(pct_frames, ignore_index=True) if pct_frames else pd.DataFrame()\n",
    "pct_path = OUT_DIR / \"percent_diffs.csv\"\n",
    "pct_all.to_csv(pct_path, index=False)\n",
    "pct_all\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d97749d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepara labels\n",
    "sv = summary.copy()\n",
    "sv[\"label\"] = sv[\"test\"] + \"—\" + sv[\"variant\"]\n",
    "\n",
    "def bar_metric(sv: pd.DataFrame, col: str, title: str, ylabel: str, fname: str):\n",
    "    if col not in sv.columns: \n",
    "        print(f\"[skip] {col} ausente no resumo.\")\n",
    "        return\n",
    "    plt.figure(figsize=(10,6))\n",
    "    plt.bar(sv[\"label\"], sv[col].values)\n",
    "    plt.xticks(rotation=45, ha=\"right\")\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.title(title)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(OUT_DIR / fname)\n",
    "    plt.close()\n",
    "\n",
    "bar_metric(sv, \"custo_unit_mean_usd_por_mm\", \"Custo unitário médio (USD por 1M)\", \"USD por 1M\",\n",
    "           \"plot_custo_unit_medio.png\")\n",
    "bar_metric(sv, \"thr_mean_rps\", \"Throughput médio (reg/s)\", \"reg/s\",\n",
    "           \"plot_throughput_medio.png\")\n",
    "bar_metric(sv, \"dur_mean_s\", \"Duração média (s)\", \"s\",\n",
    "           \"plot_duracao_media.png\")\n",
    "\n",
    "print(\"Gráficos salvos em:\", OUT_DIR.resolve())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2599d2ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flag_outliers(df_in: pd.DataFrame, group_cols, metric: str) -> pd.DataFrame:\n",
    "    rows = []\n",
    "    for keys, part in df_in.groupby(group_cols, dropna=False):\n",
    "        x = part[metric].astype(float)\n",
    "        if x.dropna().empty:\n",
    "            continue\n",
    "        q1, q3 = np.nanpercentile(x, 25), np.nanpercentile(x, 75)\n",
    "        iqr = q3 - q1\n",
    "        lo, hi = q1 - 1.5*iqr, q3 + 1.5*iqr\n",
    "        mask = (x < lo) | (x > hi)\n",
    "        cols = [c for c in [\"job_name\",\"run\"] if c in part.columns]\n",
    "        tmp = part.loc[mask, group_cols + [metric] + cols].copy()\n",
    "        if not tmp.empty:\n",
    "            tmp[\"lo\"], tmp[\"hi\"] = lo, hi\n",
    "            rows.append(tmp)\n",
    "    return pd.concat(rows, ignore_index=True) if rows else pd.DataFrame()\n",
    "\n",
    "out_dur = flag_outliers(df, [\"test\",\"variant\"], \"duration_s\") if \"duration_s\" in df.columns else pd.DataFrame()\n",
    "out_thr = flag_outliers(df, [\"test\",\"variant\"], \"throughput_rps\") if \"throughput_rps\" in df.columns else pd.DataFrame()\n",
    "out_cum = flag_outliers(df, [\"test\",\"variant\"], \"unit_cost_usd_per_mm\") if \"unit_cost_usd_per_mm\" in df.columns else pd.DataFrame()\n",
    "\n",
    "out_dur.to_csv(OUT_DIR / \"possiveis_outliers_duracao.csv\", index=False)\n",
    "out_thr.to_csv(OUT_DIR / \"possiveis_outliers_throughput.csv\", index=False)\n",
    "out_cum.to_csv(OUT_DIR / \"possiveis_outliers_custo_unit.csv\", index=False)\n",
    "\n",
    "out_dur.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "872b7e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "eff = df.copy()\n",
    "# Frações de custo por componente (quando disponíveis)\n",
    "for col in [\"cost_vcpu_usd\",\"cost_memory_usd\",\"cost_shuffle_usd\",\"total_cost_usd\"]:\n",
    "    if col in eff.columns:\n",
    "        eff[col] = pd.to_numeric(eff[col], errors=\"coerce\")\n",
    "\n",
    "if {\"cost_vcpu_usd\",\"cost_memory_usd\",\"cost_shuffle_usd\",\"total_cost_usd\"}.issubset(eff.columns):\n",
    "    eff[\"cost_vcpu_frac\"]   = eff[\"cost_vcpu_usd\"]   / eff[\"total_cost_usd\"]\n",
    "    eff[\"cost_memory_frac\"] = eff[\"cost_memory_usd\"] / eff[\"total_cost_usd\"]\n",
    "    eff[\"cost_shuffle_frac\"]= eff[\"cost_shuffle_usd\"]/ eff[\"total_cost_usd\"]\n",
    "\n",
    "# $ por vCPU-h e $ por GB-h de memória\n",
    "if {\"total_cost_usd\",\"vcpu_hours\"}.issubset(eff.columns):\n",
    "    eff[\"usd_per_vcpu_h\"] = eff[\"total_cost_usd\"] / eff[\"vcpu_hours\"]\n",
    "if {\"total_cost_usd\",\"memory_gb_hours\"}.issubset(eff.columns):\n",
    "    eff[\"usd_per_mem_gb_h\"] = eff[\"total_cost_usd\"] / eff[\"memory_gb_hours\"]\n",
    "\n",
    "# $ por GB de shuffle\n",
    "if {\"total_cost_usd\",\"shuffle_gb\"}.issubset(eff.columns):\n",
    "    eff[\"usd_per_shuffle_gb\"] = eff[\"total_cost_usd\"] / eff[\"shuffle_gb\"]\n",
    "\n",
    "# Agregamento por Teste–Variante\n",
    "kpis = [\"usd_per_vcpu_h\",\"usd_per_mem_gb_h\",\"usd_per_shuffle_gb\",\n",
    "        \"cost_vcpu_frac\",\"cost_memory_frac\",\"cost_shuffle_frac\"]\n",
    "present_kpis = [k for k in kpis if k in eff.columns]\n",
    "\n",
    "if present_kpis:\n",
    "    eff_summary = eff.groupby([\"test\",\"variant\"], dropna=False)[present_kpis].mean().reset_index()\n",
    "    eff_path = OUT_DIR / \"eficiencia_recursos_teste_variante.csv\"\n",
    "    eff_summary.to_csv(eff_path, index=False)\n",
    "    eff_summary\n",
    "else:\n",
    "    print(\"Sem colunas de KPIs\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1de3815",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(469.35096158333334 - 468.9694518333333)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
