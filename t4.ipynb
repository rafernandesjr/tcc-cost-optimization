{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# GCP\n",
    "PROJECT_ID   = \"fifth-medley-468408-a2\"\n",
    "REGION       = \"southamerica-east1\"\n",
    "ZONE         = \"southamerica-east1-b\"\n",
    "BUCKET       = \"bucket-mba-ricardo\"\n",
    "FOLDER       = \"t4\"\n",
    "BASE_IN = f\"gs://{BUCKET}/{FOLDER}\"\n",
    "BASE_OUT = f\"gs://{BUCKET}/{FOLDER}/out\"\n",
    "\n",
    "# Local\n",
    "LOCAL_FOLDER = 'dados'\n",
    "DESNORM_LOCAL_FILES = {\n",
    "    \"base_desnormalizada_5mb.csv\",\n",
    "    \"base_desnormalizada_50mb.csv\",\n",
    "    \"base_desnormalizada_500mb.csv\",\n",
    "    \"base_desnormalizada_1gb.csv\"\n",
    "}\n",
    "NORM_LOCAL_FILES = {\n",
    "    \"base_normalizada_5mb/\",\n",
    "    \"base_normalizada_50mb/\",\n",
    "    \"base_normalizada_500mb/\",\n",
    "    \"base_normalizada_1gb/\"\n",
    "}\n",
    "\n",
    "# Job Execution\n",
    "RUNS_CSV = Path(f\"runs_{FOLDER}.csv\")\n",
    "REPS = 3\n",
    "HAS_HEADER = True\n",
    "MAX_WALLTIME_SECONDS = 3600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re\n",
    "\n",
    "def slugify(s: str) -> str:\n",
    "    s = s.strip().lower().replace(\"_\", \"-\")\n",
    "    s = re.compile(r\"[^a-z0-9-]+\").sub(\"-\", s)\n",
    "    s = re.sub(r\"-{2,}\", \"-\", s).strip(\"-\")\n",
    "    return s[:63]\n",
    "\n",
    "def dataset_tag_from_uri(uri: str) -> str:\n",
    "    uri = uri.rstrip(\"/\")\n",
    "    base = os.path.basename(uri)\n",
    "    stem = os.path.splitext(base)[0]\n",
    "    return slugify(stem)\n",
    "\n",
    "def build_job_name(experiment: str, variant: str, dataset_tag: str, run: int, ts) -> str:\n",
    "    name = f\"{experiment}-{variant}-{dataset_tag}-{ts}-{run}\"\n",
    "    return slugify(name)\n",
    "\n",
    "def build_output_prefix(output_subdir: str, dataset_tag: str, job_name: str) -> str:\n",
    "    return f\"{BASE_OUT}/{output_subdir}/{dataset_tag}/{job_name}/part\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import storage\n",
    "\n",
    "def prepare_gcs_inputs(\n",
    "    project_id: str,\n",
    "    bucket_name: str,\n",
    "    folder: str,\n",
    "    local_folder: str,\n",
    "    files, \n",
    "    force_upload: bool = False,\n",
    "    create_etl_folders: bool = True\n",
    "):\n",
    "    client = storage.Client(project=project_id)\n",
    "    bucket = client.bucket(bucket_name)\n",
    "    if not bucket.exists():\n",
    "        raise RuntimeError(f\"Bucket não encontrado: {bucket_name}\")\n",
    "\n",
    "    loc = (bucket.location or \"\").lower()\n",
    "\n",
    "    if create_etl_folders:\n",
    "        for etl_folder in (f\"{folder}/temp/\", f\"{folder}/staging/\", f\"{folder}/out/\"):\n",
    "            bucket.blob(etl_folder + \".keep\").upload_from_string(\"\", content_type=\"text/plain\")\n",
    "\n",
    "    input_uris = []\n",
    "\n",
    "    for item in files:\n",
    "        local_path = Path(local_folder) / item\n",
    "        if not local_path.exists():\n",
    "            raise FileNotFoundError(f\"Caminho local não encontrado: {local_path}\")\n",
    "\n",
    "        if local_path.is_dir():\n",
    "            for subfile in local_path.rglob(\"*\"):\n",
    "                if subfile.is_file():\n",
    "                    relative = subfile.relative_to(local_folder)\n",
    "                    input_blob_path = f\"{folder}/{relative.as_posix()}\"\n",
    "                    input_uri = f\"gs://{bucket_name}/{input_blob_path}\"\n",
    "                    input_blob = bucket.blob(input_blob_path)\n",
    "\n",
    "                    if input_blob.exists(client=client) and not force_upload:\n",
    "                        print(f\"Já existe no GCS: {input_uri}\")\n",
    "                    else:\n",
    "                        ctype = \"application/gzip\" if subfile.suffix == \".gz\" else \"text/csv\"\n",
    "                        input_blob.upload_from_filename(subfile.as_posix(), content_type=ctype)\n",
    "                        print(f\"Enviado: {input_uri}\")\n",
    "\n",
    "                    input_uris.append(input_uri)\n",
    "\n",
    "        else:\n",
    "            input_blob_path = f\"{folder}/{item}\"\n",
    "            input_uri = f\"gs://{bucket_name}/{input_blob_path}\"\n",
    "            input_blob = bucket.blob(input_blob_path)\n",
    "\n",
    "            if input_blob.exists(client=client) and not force_upload:\n",
    "                print(f\"Já existe no GCS: {input_uri}\")\n",
    "            else:\n",
    "                ctype = \"application/gzip\" if item.endswith(\".gz\") else \"text/csv\"\n",
    "                input_blob.upload_from_filename(local_path.as_posix(), content_type=ctype)\n",
    "                print(f\"Enviado: {input_uri}\")\n",
    "\n",
    "            input_uris.append(input_uri)\n",
    "\n",
    "    ctx = {\n",
    "        \"INPUT_URI\": input_uris,\n",
    "        \"TEMP_GCS\":  f\"gs://{bucket_name}/{folder}/temp\",\n",
    "        \"STAGING_GCS\": f\"gs://{bucket_name}/{folder}/staging\",\n",
    "        \"OUT_BASE\":  f\"gs://{bucket_name}/{folder}/out\",\n",
    "        \"bucket_location\": loc,\n",
    "    }\n",
    "    print(\"Prefixos prontos em:\", ctx[\"TEMP_GCS\"], ctx[\"STAGING_GCS\"], ctx[\"OUT_BASE\"])\n",
    "    return ctx, input_uris\n",
    "\n",
    "\n",
    "norm_ctx, NORM_INPUT_URIS = prepare_gcs_inputs(\n",
    "    PROJECT_ID, BUCKET, FOLDER,\n",
    "    local_folder=LOCAL_FOLDER,\n",
    "    files=NORM_LOCAL_FILES\n",
    ")\n",
    "desnorm_ctx, DESNORM_INPUT_URIS = prepare_gcs_inputs(\n",
    "    PROJECT_ID, BUCKET, FOLDER,\n",
    "    local_folder=LOCAL_FOLDER,\n",
    "    files=DESNORM_LOCAL_FILES\n",
    ")\n",
    "print(NORM_INPUT_URIS)\n",
    "print(DESNORM_INPUT_URIS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd, subprocess, datetime as dt\n",
    "\n",
    "def sh(cmd):\n",
    "    print(\">>\", cmd)\n",
    "    result = subprocess.run(cmd, shell=True, text=True, capture_output=True)\n",
    "    if result.returncode != 0:\n",
    "        print(result.stdout)\n",
    "        print(result.stderr)\n",
    "        raise RuntimeError(f\"Falhou: {cmd}\")\n",
    "    return result\n",
    "\n",
    "def append_run(experiment, variant, job_name, start_ts, end_ts, extra=None):\n",
    "    row = {\n",
    "        \"timestamp_utc\": (dt.datetime.utcnow()).isoformat(timespec=\"seconds\"),\n",
    "        \"experiment\": experiment,\n",
    "        \"variant\": variant,\n",
    "        \"job_name\": job_name,\n",
    "        \"start_ts\": start_ts.isoformat(timespec=\"seconds\") if start_ts else None,\n",
    "        \"end_ts\":   end_ts.isoformat(timespec=\"seconds\")   if end_ts else None,\n",
    "        \"duration_s\": (end_ts - start_ts).total_seconds() if (start_ts and end_ts) else None,\n",
    "        **(extra or {})\n",
    "    }\n",
    "    df = pd.DataFrame([row])\n",
    "    if RUNS_CSV.exists():\n",
    "        base = pd.read_csv(RUNS_CSV)\n",
    "        df = pd.concat([base, df], ignore_index=True)\n",
    "    df.to_csv(RUNS_CSV, index=False)\n",
    "    print(\"Job Registrado:\", job_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, time\n",
    "\n",
    "def convert_iso_do_dt(s):\n",
    "    if not s:\n",
    "        return None\n",
    "    s = s.replace('Z', '+00:00')\n",
    "    d = dt.datetime.fromisoformat(s)\n",
    "    return d\n",
    "\n",
    "def get_dataflow_job_id_by_name(job_name: str) -> str | None:\n",
    "    command = f\"gcloud dataflow jobs list --region={REGION} --format=json --filter=name={job_name}\"\n",
    "    out = sh(command).stdout\n",
    "    arr = json.loads(out or \"[]\")\n",
    "    return arr[0].get(\"id\") if arr else None\n",
    "\n",
    "def wait_dataflow_job(job_id: str, poll_s: int = 20):\n",
    "    \"\"\"Espera o job finalizar. Retorna (state, start_dt, end_dt).\"\"\"\n",
    "    terminal = {\"JOB_STATE_DONE\",\"JOB_STATE_FAILED\",\"JOB_STATE_CANCELLED\",\"JOB_STATE_DRAINED\"}\n",
    "    while True:\n",
    "        info = json.loads(sh(f\"gcloud dataflow jobs describe {job_id} --region={REGION} --format=json\").stdout or \"{}\")\n",
    "        state = info.get(\"currentState\")\n",
    "        create_time = convert_iso_do_dt(info.get(\"createTime\"))\n",
    "        end_time    = convert_iso_do_dt(info.get(\"currentStateTime\")) if state in terminal else None\n",
    "        if state in terminal and end_time and create_time:\n",
    "            return state, create_time, end_time\n",
    "        time.sleep(poll_s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.parse import urlparse\n",
    "import gzip, re\n",
    "\n",
    "def convert_local_to_gs(uri: str):\n",
    "    assert uri.startswith(\"gs://\")\n",
    "    u = urlparse(uri.replace(\"gs://\", \"gs://dummy/\"))\n",
    "    return u.netloc, u.path[1:]\n",
    "\n",
    "def count_records_any(uri: str, has_header=True) -> int:\n",
    "    bkt, path = convert_local_to_gs(uri)\n",
    "    client = storage.Client(project=PROJECT_ID)\n",
    "    bucket = client.bucket(bkt)\n",
    "\n",
    "    if \"*\" in path:\n",
    "        prefix = path.split(\"*\", 1)[0]\n",
    "        blobs = [b for b in bucket.list_blobs(prefix=prefix) if re.fullmatch(path.replace(\"*\", \".*\"), b.name)]\n",
    "    else:\n",
    "        blobs = [bucket.blob(path)]\n",
    "    \n",
    "    total = 0\n",
    "    for blob in blobs:\n",
    "        with blob.open(\"rb\") as base:\n",
    "            reader = gzip.GzipFile(fileobj=base) if blob.name.endswith(\".gz\") else base\n",
    "            while True:\n",
    "                chunk = reader.read(1024 * 1024)\n",
    "                if not chunk: break\n",
    "                total += chunk.count(b\"\\n\")\n",
    "        if has_header and total > 0:\n",
    "            total -= 1\n",
    "    return max(total, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Values from southamerica-east1 - 2025-09\n",
    "PRICE_BATCH_VCPU_HOUR   = 0.084     # USD/1 vCPU hour\n",
    "PRICE_BATCH_MEMORY_GB_HOUR = 0.0053355  # USD/1 GB hour\n",
    "PRICE_BATCH_SHUFFLE_GB  = 0.01747   # USD/1 GB\n",
    "\n",
    "PRICE_FLEXRS_VCPU_HOUR   = 0.0534   # USD/1 vCPU hour\n",
    "PRICE_FLEXRS_MEMORY_GB_HOUR = 0.0033889  # USD/1 GB hour\n",
    "PRICE_FLEXRS_SHUFFLE_GB  = 0.01747  # USD/1 GB\n",
    "\n",
    "def get_type_job(job_info: dict) -> str:\n",
    "    job_type = job_info.get(\"type\")\n",
    "    env = job_info.get(\"environment\", {}) or {}\n",
    "    flexrs_goal = env.get(\"flexResourceSchedulingGoal\")\n",
    "    version_job_type = (env.get(\"version\") or {}).get(\"job_type\")\n",
    "    if job_type == \"JOB_TYPE_STREAMING\":\n",
    "        return \"streaming\"\n",
    "    if version_job_type == \"FNAPI_BATCH\":\n",
    "        return \"serverless\"\n",
    "    if flexrs_goal:\n",
    "        return \"flexrs\"\n",
    "    return \"batch\"\n",
    "\n",
    "def calcular_custos(project_id, region, job_id):\n",
    "\n",
    "    # Describe\n",
    "    describe_cmd = f\"gcloud dataflow jobs describe {job_id} --region={region} --format=json --project={project_id}\"\n",
    "    job_info = json.loads(sh(describe_cmd).stdout)\n",
    "\n",
    "    start = dt.datetime.fromisoformat(job_info[\"createTime\"].replace(\"Z\", \"+00:00\"))\n",
    "    end   = dt.datetime.fromisoformat(job_info[\"currentStateTime\"].replace(\"Z\", \"+00:00\"))\n",
    "    duration_s = (end - start).total_seconds()\n",
    "\n",
    "    type_job = get_type_job(job_info)\n",
    "    if type_job == \"flexrs\":\n",
    "        price_vcpu_hour, price_memory_gb_hour, price_shuffle_gb = PRICE_FLEXRS_VCPU_HOUR, PRICE_FLEXRS_MEMORY_GB_HOUR, PRICE_FLEXRS_SHUFFLE_GB\n",
    "    else:\n",
    "        price_vcpu_hour, price_memory_gb_hour, price_shuffle_gb = PRICE_BATCH_VCPU_HOUR, PRICE_BATCH_MEMORY_GB_HOUR, PRICE_BATCH_SHUFFLE_GB\n",
    "\n",
    "    # Resource metrics\n",
    "    metrics_cmd = f\"gcloud beta dataflow metrics list {job_id} --region={region} --format=json --project={project_id}\"\n",
    "    metrics_info = json.loads(sh(metrics_cmd).stdout or \"[]\")\n",
    "\n",
    "    vcpu_hours = 0.0\n",
    "    memory_gb_hours = 0.0\n",
    "    pd_gb_hours = 0.0\n",
    "    ssd_gb_hours = 0.0\n",
    "    shuffle_gb = 0.0\n",
    "    total_shuffle_gb = 0.0\n",
    "    current_vcpus = None\n",
    "    current_memory_gb = None\n",
    "    records_processed = 0\n",
    "\n",
    "    for m in metrics_info:\n",
    "        name = (m.get(\"name\") or {}).get(\"name\")\n",
    "        val = float(m.get(\"scalar\", 0) or 0)\n",
    "\n",
    "        if name == \"TotalVcpuTime\":\n",
    "            vcpu_hours = val / 3600.0\n",
    "        elif name == \"TotalMemoryUsage\":\n",
    "            memory_gb_hours = (val / 1024.0) / 3600.0\n",
    "        elif name == \"TotalPdUsage\":\n",
    "            pd_gb_hours = val / 3600.0\n",
    "        elif name == \"TotalSsdUsage\":\n",
    "            ssd_gb_hours = val / 3600.0\n",
    "        elif name == \"BillableShuffleDataProcessed\":\n",
    "            shuffle_gb = val\n",
    "        elif name == \"TotalShuffleDataProcessed\":\n",
    "            total_shuffle_gb = val\n",
    "        elif name == \"CurrentVcpuCount\":\n",
    "            current_vcpus = int(val)\n",
    "        elif name == \"CurrentMemoryUsage\":\n",
    "            current_memory_gb = val / 1024.0\n",
    "        elif name == \"ElementCount\":\n",
    "            records_processed = max(records_processed, int(val))\n",
    "        elif name in (\"rows_in\", \"rows_out\"):\n",
    "            records_processed = max(records_processed, int(val))\n",
    "\n",
    "    throughput = (records_processed / duration_s) if (duration_s and records_processed) else None\n",
    "    cost_vcpu = vcpu_hours * price_vcpu_hour\n",
    "    cost_memory = memory_gb_hours * price_memory_gb_hour\n",
    "    cost_shuffle = shuffle_gb * price_shuffle_gb\n",
    "    total_cost_usd = cost_vcpu + cost_memory + cost_shuffle\n",
    "    unit_cost_usd_per_mm = (total_cost_usd / (records_processed / 1e6)) if records_processed else None\n",
    "\n",
    "    return {\n",
    "        \"job_id\": job_id,\n",
    "        \"type_job\": type_job,\n",
    "        \"duration_s\": duration_s,\n",
    "        \"vcpu_hours\": vcpu_hours,\n",
    "        \"memory_gb_hours\": memory_gb_hours,\n",
    "        \"pd_gb_hours\": pd_gb_hours,\n",
    "        \"ssd_gb_hours\": ssd_gb_hours,\n",
    "        \"shuffle_gb\": shuffle_gb,\n",
    "        \"total_shuffle_gb\": total_shuffle_gb,\n",
    "        \"current_vcpus\": current_vcpus,\n",
    "        \"current_memory_gb\": current_memory_gb,\n",
    "        \"records_processed\": records_processed,\n",
    "        \"throughput_rps\": throughput,\n",
    "        \"cost_vcpu_usd\": cost_vcpu,\n",
    "        \"cost_memory_usd\": cost_memory,\n",
    "        \"cost_shuffle_usd\": cost_shuffle,\n",
    "        \"total_cost_usd\": total_cost_usd,\n",
    "        \"unit_cost_usd_per_mm\": unit_cost_usd_per_mm,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Dataflow\n",
    "\n",
    "def apply_limit(s: str) -> str:\n",
    "    s = s.lower()\n",
    "    s = re.sub(r'[^a-z0-9_-]', '-', s)\n",
    "    return s[:63]\n",
    "\n",
    "def run_dataflow(\n",
    "    experiment: str,\n",
    "    variant: str,\n",
    "    run_id: int,\n",
    "    input_uri: str,\n",
    "    output_subdir: str,\n",
    "    aditional_flags: list[str] | None = None,\n",
    "    autoscaling_algorithm: str | None = \"THROUGHPUT_BASED\",\n",
    "    num_workers: int | None = 1,\n",
    "    max_num_workers: int | None = 4,\n",
    "    worker_machine_type: str | None = \"n2-standard-2\",\n",
    "    iterations: int | None = None,\n",
    "    flexrs_goal: str | None = None, # \"SPEED_OPTIMIZED\"|\"COST_OPTIMIZED\"\n",
    "):\n",
    "\n",
    "    dataset_tag = dataset_tag_from_uri(input_uri)\n",
    "    timestamp  = (dt.datetime.utcnow()).strftime('%Y%m%d-%H%M%S')\n",
    "    job = build_job_name(experiment.lower(), variant, dataset_tag, timestamp, run_id)\n",
    "    output_prefix = build_output_prefix(output_subdir, dataset_tag, job)\n",
    "\n",
    "    label_flags = [\n",
    "        f'--labels=experiment={apply_limit(experiment)}',\n",
    "        f'--labels=variant={apply_limit(variant)}',\n",
    "        f'--labels=run={apply_limit(str(run_id))}',\n",
    "        f'--labels=folder={apply_limit(FOLDER)}',\n",
    "        f\"--labels=dataset={apply_limit(dataset_tag)}\",\n",
    "        f'--labels=job={apply_limit(job)}',\n",
    "    ]\n",
    "\n",
    "    flags = [\n",
    "        f'--runner=DataflowRunner',\n",
    "        f'--project={PROJECT_ID}',\n",
    "        f'--region={REGION}',\n",
    "        f\"--temp_location=gs://{BUCKET}/{FOLDER}/temp\",\n",
    "        f\"--staging_location=gs://{BUCKET}/{FOLDER}/staging\",\n",
    "        f'--job_name={job}',\n",
    "        f'--input={input_uri}',\n",
    "        f'--output={output_prefix}',\n",
    "        f'--autoscaling_algorithm={autoscaling_algorithm}',\n",
    "        f\"--num_workers={num_workers}\",\n",
    "        f\"--max_num_workers={max_num_workers}\",\n",
    "        f\"--worker_machine_type={worker_machine_type}\",\n",
    "        f'--no_pipeline_monitoring',\n",
    "        f'--dataflow_service_options=max_workflow_runtime_walltime_seconds={MAX_WALLTIME_SECONDS}',\n",
    "        *label_flags\n",
    "    ]\n",
    "\n",
    "    if HAS_HEADER:\n",
    "        flags += [f'--has_header']\n",
    "    if iterations:\n",
    "        flags += [f'--iterations={iterations}']\n",
    "    if flexrs_goal:\n",
    "        flags += [f'--flexrs_goal={flexrs_goal}']\n",
    "\n",
    "    if aditional_flags:    \n",
    "        flags += aditional_flags\n",
    "\n",
    "    cmd = \"python mbapipeline.py \" + \" \".join(flags)\n",
    "    sh(cmd)\n",
    "\n",
    "    job_id = get_dataflow_job_id_by_name(job)\n",
    "    if not job_id:\n",
    "        raise RuntimeError(\"Não foi possível obter o job_id pelo nome.\")\n",
    "    state, t0, t1 = wait_dataflow_job(job_id)\n",
    "\n",
    "    duration_s = (t1 - t0).total_seconds()\n",
    "\n",
    "    try:\n",
    "        metrics_cost = calcular_custos(PROJECT_ID, REGION, job_id)\n",
    "    except Exception as e:\n",
    "        print(\"Falha ao calcular as métricas\", e)\n",
    "        metrics_cost = {}\n",
    "\n",
    "    append_run(experiment, variant, job, t0, t1, {\n",
    "        \"runner\": \"dataflow\",\n",
    "        \"job_id\": job_id,\n",
    "        \"state\": state,\n",
    "        \"duration_s\": duration_s,\n",
    "        \"worker_type\": worker_machine_type,\n",
    "        \"num_workers\": num_workers,\n",
    "        \"max_num_workers\": max_num_workers,\n",
    "        \"flexrs_goal\": flexrs_goal,\n",
    "        \"out_prefix\": output_prefix,\n",
    "        \"input_uri\": input_uri,\n",
    "        \"dataset_tag\": dataset_tag,\n",
    "        **metrics_cost\n",
    "    })\n",
    "    return job, output_prefix, job_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NORM_INPUT_URIS = [\n",
    "    \"gs://bucket-mba-ricardo/t4/base_normalizada_5mb/\",\n",
    "    \"gs://bucket-mba-ricardo/t4/base_normalizada_500mb/\",\n",
    "    \"gs://bucket-mba-ricardo/t4/base_normalizada_1gb/\",\n",
    "    \"gs://bucket-mba-ricardo/t4/base_normalizada_50mb/\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(DESNORM_INPUT_URIS)\n",
    "print(NORM_INPUT_URIS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for uri in DESNORM_INPUT_URIS:\n",
    "    for i in range(1, REPS + 1):\n",
    "        run_dataflow(\n",
    "            \"T4\", \"desnormalizada\", i,\n",
    "            input_uri=uri,\n",
    "            output_subdir=\"t4-desnormalizada\",\n",
    "            num_workers=1, max_num_workers=4,\n",
    "            worker_machine_type=\"n2-standard-4\",\n",
    "            iterations=100,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for uri in NORM_INPUT_URIS:\n",
    "    for i in range(1, REPS + 1):\n",
    "        run_dataflow(\n",
    "            \"T4\", \"normalizada\", i,\n",
    "            input_uri=uri,\n",
    "            output_subdir=\"t4-normalizada\",\n",
    "            num_workers=1, max_num_workers=4,\n",
    "            worker_machine_type=\"n2-standard-4\",\n",
    "            iterations=100,\n",
    "        )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
